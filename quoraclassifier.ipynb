{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MJIINMVl2q9J"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gSmy6TC4to3"},"outputs":[],"source":["df=pd.read_csv(\"/content/train.csv\")\n","df=df.head(100000)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10347,"status":"ok","timestamp":1688812097914,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"0Xa13gfbzVR2","outputId":"1883f611-b9be-4a77-bd34-47c1a9fdf1b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}],"source":["pip install nltk"]},{"cell_type":"markdown","metadata":{"id":"6-INYv0pDkij"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":677,"status":"ok","timestamp":1688812098535,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"SG_XrxBs49Yo","outputId":"2e60aa9e-ea42-4387-f43f-8d0ee87cff10"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-a3544950-5584-4b23-bc87-e0c2334c0a03\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>qid1</th>\n","      <th>qid2</th>\n","      <th>question1</th>\n","      <th>question2</th>\n","      <th>is_duplicate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>99995</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>99996</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>99997</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>99998</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>99999</th>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100000 rows Ã— 6 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3544950-5584-4b23-bc87-e0c2334c0a03')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a3544950-5584-4b23-bc87-e0c2334c0a03 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a3544950-5584-4b23-bc87-e0c2334c0a03');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["          id   qid1   qid2  question1  question2  is_duplicate\n","0      False  False  False      False      False         False\n","1      False  False  False      False      False         False\n","2      False  False  False      False      False         False\n","3      False  False  False      False      False         False\n","4      False  False  False      False      False         False\n","...      ...    ...    ...        ...        ...           ...\n","99995  False  False  False      False      False         False\n","99996  False  False  False      False      False         False\n","99997  False  False  False      False      False         False\n","99998  False  False  False      False      False         False\n","99999  False  False  False      False      False         False\n","\n","[100000 rows x 6 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df.isnull()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1688812098536,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"GsixzIp0DnIr","outputId":"cb6f5125-5527-4f9d-9c05-8494397fe549"},"outputs":[{"name":"stdout","output_type":"stream","text":["id              0\n","qid1            0\n","qid2            0\n","question1       0\n","question2       0\n","is_duplicate    0\n","dtype: int64\n"]}],"source":["missing_values = df.isna().sum()\n","\n","print(missing_values)"]},{"cell_type":"markdown","metadata":{"id":"p1zmPxV7ELLK"},"source":["Thus no need to handle missing values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1688812098536,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"BNWUIJ8bENqB","outputId":"1dc11e51-1d9d-4c47-d9c2-b7505efd6717"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 100000 entries, 0 to 99999\n","Data columns (total 6 columns):\n"," #   Column        Non-Null Count   Dtype \n","---  ------        --------------   ----- \n"," 0   id            100000 non-null  int64 \n"," 1   qid1          100000 non-null  int64 \n"," 2   qid2          100000 non-null  int64 \n"," 3   question1     100000 non-null  object\n"," 4   question2     100000 non-null  object\n"," 5   is_duplicate  100000 non-null  int64 \n","dtypes: int64(4), object(2)\n","memory usage: 4.6+ MB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"15TT35h-h48m"},"source":["#ADDING CUSTOM FEATURES"]},{"cell_type":"markdown","metadata":{"id":"7PLmOlAFeLqM"},"source":["**csc_min=#common stopwords/min of stopwords in both questions and also csc_max**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xABc42uxZyZo"},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","\n","def csc_min(data):\n","    try:\n","        # nltk.download('stopwords')\n","        # nltk.download('punkt')\n","        # Get the list of stopwords for English\n","        stop_words = set(stopwords.words('english'))\n","\n","        # Tokenize the texts into words\n","        words1 = nltk.word_tokenize(data[\"question1\"])\n","        words2 = nltk.word_tokenize(data[\"question2\"])\n","\n","        # Convert the words to lowercase\n","        words1 = [str(word.lower()) for word in words1]\n","        words2 = [str(word.lower()) for word in words2]\n","\n","        # Find the common stopwords\n","        common_stopwords = set(words1) & set(words2)\n","\n","        # Count the number of common stopwords\n","        num_common_stopwords = len(common_stopwords.intersection(stop_words))\n","    except (TypeError, KeyError):\n","        print(data)\n","        # Handle the case where the data is not of the expected type or the expected keys are missing\n","        return 0\n","\n","    num_stopwords1 = sum(word in stop_words for word in words1)\n","    num_stopwords2 = sum(word in stop_words for word in words2)\n","    min_len=min(num_stopwords1,num_stopwords2)\n","\n","    return num_common_stopwords/min_len if min_len>0 else num_common_stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQ9MlMftC4qH"},"outputs":[],"source":["df[\"csc_min\"]=df.apply(csc_min,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9CUzbIsilua"},"outputs":[],"source":["def csc_max(data):\n","    try:\n","        # nltk.download('stopwords')\n","        # nltk.download('punkt')\n","        # Get the list of stopwords for English\n","        stop_words = set(stopwords.words('english'))\n","\n","        # Tokenize the texts into words\n","        words1 = nltk.word_tokenize(data[\"question1\"])\n","        words2 = nltk.word_tokenize(data[\"question2\"])\n","\n","        # Convert the words to lowercase\n","        words1 = [str(word.lower()) for word in words1]\n","        words2 = [str(word.lower()) for word in words2]\n","\n","        # Find the common stopwords\n","        common_stopwords = set(words1) & set(words2)\n","\n","        # Count the number of common stopwords\n","        num_common_stopwords = len(common_stopwords.intersection(stop_words))\n","    except (TypeError, KeyError):\n","        print(data)\n","        # Handle the case where the data is not of the expected type or the expected keys are missing\n","        return 0\n","\n","    num_stopwords1 = sum(word in stop_words for word in words1)\n","    num_stopwords2 = sum(word in stop_words for word in words2)\n","    max_len=max(num_stopwords1,num_stopwords2)\n","\n","    return num_common_stopwords/max_len if max_len>0 else num_common_stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDwd43O-jbcc"},"outputs":[],"source":["df[\"csc_max\"]=df.apply(csc_max,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"dmJrZY6iiUxk"},"source":["**CTC-min= ratio of number of common tokens to number of tokens in smaller question CTC-max= ratio of number of common tokens to number of tokens in larger question bold text**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_R8yDLdmikMl"},"outputs":[],"source":["def ctc_min(data):\n","      try:\n","          words1 = nltk.word_tokenize(data[\"question1\"])\n","          words2 = nltk.word_tokenize(data[\"question2\"])\n","\n","          # Convert the words to lowercase\n","          words1 = [str(word.lower()) for word in words1]\n","          words2 = [str(word.lower()) for word in words2]\n","\n","          # Find the common words\n","          common_words = np.intersect1d(words1, words2)\n","          # Get the count of common words\n","          num_common_words = len(common_words)\n","\n","          min_len=min(len(words1),len(words2))\n","\n","          if min_len==0:\n","            ctc_min=num_common_words/1\n","\n","          else:\n","            ctc_min=num_common_words/min_len\n","\n","          return ctc_min\n","\n","      except (TypeError, KeyError):\n","          print(data)\n","          # Handle the case where the data is not of the expected type or the expected keys are missing\n","          return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjKKPefLiog-"},"outputs":[],"source":["df['ctc_min']=df.apply(ctc_min,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUPwI1mTjibZ"},"outputs":[],"source":["def ctc_max(data):\n","      try:\n","          words1 = nltk.word_tokenize(data[\"question1\"])\n","          words2 = nltk.word_tokenize(data[\"question2\"])\n","\n","          # Convert the words to lowercase\n","          words1 = [str(word.lower()) for word in words1]\n","          words2 = [str(word.lower()) for word in words2]\n","\n","          # Find the common words\n","          common_words = np.intersect1d(words1, words2)\n","          # Get the count of common words\n","          num_common_words = len(common_words)\n","\n","          max_len=max(len(words1),len(words2))\n","\n","          if max_len==0:\n","            ctc_max=num_common_words/1\n","\n","          else:\n","            ctc_max=num_common_words/max_len\n","\n","          return ctc_max\n","\n","      except (TypeError, KeyError):\n","          print(data)\n","          # Handle the case where the data is not of the expected type or the expected keys are missing\n","          return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GlIEteRjwOt"},"outputs":[],"source":["df['ctc_max']=df.apply(ctc_max,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"BeSUTUZZG9Ld"},"source":["**Adding longest substr ratio feature where it is equal to length of common longest substring divided by length of smaller question**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10pysHu_HMTD"},"outputs":[],"source":["def longest_substr(data):\n","  try:\n","   words1 = nltk.word_tokenize(data[\"question1\"])\n","   words2 = nltk.word_tokenize(data[\"question2\"])\n","\n","    # Convert the words to lowercase\n","   words1 = [str(word.lower()) for word in words1]\n","   words2 = [str(word.lower()) for word in words2]\n","\n","   count=0\n","\n","   for i in range(len(words1)):\n","       for j in range(len(words1)):\n","          new_list=words1[i:j]\n","          for l in range(len(words2)):\n","             if words2[l:l+j-i]==new_list and j-i>count:\n","                count=j-i\n","\n","   #finding min length's question\n","   min_len=min(len(words1),len(words2))\n","\n","   if min_len==0:\n","      longest_substr_ratio=count\n","\n","   else:\n","      longest_substr_ratio=count/min_len\n","\n","   return count\n","\n","  except (TypeError, KeyError):\n","          print(data)\n","          # Handle the case where the data is not of the expected type or the expected keys are missing\n","          return 0\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXLxrffdOXBe"},"outputs":[],"source":["df['longest_substr_ratio']=df.apply(longest_substr,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"KlYY4D96P84O"},"source":["**Adding feature of mean_length**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Llet3exQE4g"},"outputs":[],"source":["def mean_len(data):\n","   try:\n","    words1 = nltk.word_tokenize(data[\"question1\"])\n","    words2 = nltk.word_tokenize(data[\"question2\"])\n","\n","      # Convert the words to lowercase\n","    words1 = [str(word.lower()) for word in words1]\n","    words2 = [str(word.lower()) for word in words2]\n","    return ( len(words1) + len(words2)) / 2\n","   except (TypeError, KeyError):\n","          print(data)\n","          # Handle the case where the data is not of the expected type or the expected keys are missing\n","          return 0\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EiUkukU4QbQe"},"outputs":[],"source":["df['mean_length']=df.apply(mean_len,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"gCInMQOFQhkW"},"source":["**Adding feature of abs_len_diff**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z4wK7sIbQl9N"},"outputs":[],"source":["def abs_len_diff(data):\n","   import math\n","   try:\n","        words1 = nltk.word_tokenize(data[\"question1\"])\n","        words2 = nltk.word_tokenize(data[\"question2\"])\n","\n","          # Convert the words to lowercase\n","        words1 = [str(word.lower()) for word in words1]\n","        words2 = [str(word.lower()) for word in words2]\n","        diff=len(words1) - len(words2)\n","        return abs(diff)\n","\n","   except (TypeError, KeyError):\n","          print(data)\n","          # Handle the case where the data is not of the expected type or the expected keys are missing\n","          return 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrpBB6Eo4PAY"},"outputs":[],"source":["df['abs_len_diff']=df.apply(abs_len_diff,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"JiPkGx8tBiIA"},"source":["#Extracting Fuzzy Features"]},{"cell_type":"markdown","metadata":{"id":"1FkyySc8EAp1"},"source":["**(https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/)**"]},{"cell_type":"markdown","metadata":{"id":"p5t1xoigD9b5"},"source":["Adding partial ratio feature"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3805,"status":"ok","timestamp":1688812495996,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"uC0cPZBiESyn","outputId":"9f1df54c-5d4d-4cec-c120-eca257debca6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n"]}],"source":["pip install fuzzywuzzy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1688812495997,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"XHXvWPIbBpH3","outputId":"04122ae0-8bff-4c10-8fc9-2fb57576d7dc"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n","  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"]}],"source":["from fuzzywuzzy import fuzz\n","def partial_ratio(data):\n","  try:\n","      text1 = data[\"question1\"]\n","      text2 = data[\"question2\"]\n","      return fuzz.partial_ratio(str(text1).lower(),str(text2).lower())\n","  except (TypeError, KeyError):\n","          print(data)\n","          # Handle the case where the data is not of the expected type or the expected keys are missing\n","          return 0\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfSnpGoFExz_"},"outputs":[],"source":["df['partial_ratio']=df.apply(partial_ratio,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"C9D4_elZE9CP"},"source":["Adding token sort ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGEEhyIwE8Vh"},"outputs":[],"source":["def token_sort_ratio(data):\n","  try:\n","      text1 = data[\"question1\"]\n","      text2 = data[\"question2\"]\n","      return fuzz.token_sort_ratio(str(text1).lower(),str(text2).lower())\n","  except (TypeError, KeyError):\n","          print(data)\n","          # Handle the case where the data is not of the expected type or the expected keys are missing\n","          return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqe4I63eFQQI"},"outputs":[],"source":["df['token_sort_ratio']=df.apply(token_sort_ratio,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"UQuzdn01FXIH"},"source":["Adding token set ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53KwEeJJFZcv"},"outputs":[],"source":["def token_set_ratio(data):\n","  try:\n","      text1 = data[\"question1\"]\n","      text2 = data[\"question2\"]\n","      return fuzz.token_set_ratio(str(text1).lower(),str(text2).lower())\n","  except (TypeError, KeyError):\n","          print(data)\n","          # Handle the case where the data is not of the expected type or the expected keys are missing\n","          return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NgKLpC3qFdSP"},"outputs":[],"source":["df['token_set_ratio']=df.apply(token_set_ratio,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"yflKQHjfkDQH"},"source":["**CWC-min= ratio of number of common words to number of words in smaller question CWC-max= ratio of number of common words to number of words in larger question bold text**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CH1qkgDckQ6O"},"outputs":[],"source":["def cwc_min(data):\n","      try:\n","        import re\n","\n","        stop_words = set(stopwords.words('english'))\n","\n","        # Define the pattern for punctuation marks\n","        punctuation_pattern = r'[^\\w\\s]'\n","\n","        # Remove punctuation marks using regular expression substitution\n","        # Example texts\n","        text1 = data[\"question1\"]\n","        text2 = data[\"question2\"]\n","        text1 = re.sub(punctuation_pattern, ' ', text1)\n","        text2 = re.sub(punctuation_pattern, ' ', text2)\n","        # Tokenize the texts into words\n","        words1 = str(text1).lower().split()\n","        words2 = str(text2).lower().split()\n","\n","        # Find the common words\n","        common_words = np.intersect1d(words1, words2)\n","\n","        print(common_words)\n","        # Get the count of common words\n","        num_common_words = len(common_words)\n","\n","        #BUT ACTUALLY HERE WORDS MEAN TOKENS OR SIMPLE WORDS-STOP WORDS SO WE DO LIKE THIS\n","        common_stopwords = set(words1) & set(words2)\n","\n","        print(common_stopwords)\n","        # Count the number of common stopwords\n","        num_common_stopwords = len(common_stopwords.intersection(stop_words))\n","\n","        num_common_words=num_common_words-num_common_stopwords\n","\n","        #get min number of words' question\n","        num_stopwords1 = sum(word in stop_words for word in words1)\n","        num_stopwords2 = sum(word in stop_words for word in words2)\n","\n","        min_len=min( len(str(text1).lower().split()) - num_stopwords1,\n","                    len(str(text2).lower().split()) - num_stopwords2)\n","        if min_len==0:\n","          cwc_min=num_common_words/1\n","        else:\n","          cwc_min=num_common_words/min_len\n","\n","        return cwc_min\n","      except (TypeError, KeyError):\n","        print(data)\n","        # Handle the case where the data is not of the expected type or the expected keys are missing\n","        return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wa3nOlTJmaAG"},"outputs":[],"source":["df['cwc_min']=df.apply(ctc_min,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcpWmt-JkAzz"},"outputs":[],"source":["def cwc_max(data):\n","      try:\n","        import re\n","\n","        stop_words = set(stopwords.words('english'))\n","\n","        # Define the pattern for punctuation marks\n","        punctuation_pattern = r'[^\\w\\s]'\n","\n","        # Remove punctuation marks using regular expression substitution\n","        # Example texts\n","        text1 = data[\"question1\"]\n","        text2 = data[\"question2\"]\n","        text1 = re.sub(punctuation_pattern, ' ', text1)\n","        text2 = re.sub(punctuation_pattern, ' ', text2)\n","        # Tokenize the texts into words\n","        words1 = str(text1).lower().split()\n","        words2 = str(text2).lower().split()\n","\n","        # Find the common words\n","        common_words = np.intersect1d(words1, words2)\n","\n","        # Get the count of common words\n","        num_common_words = len(common_words)\n","\n","        #BUT ACTUALLY HERE WORDS MEAN TOKENS OR SIMPLE WORDS-STOP WORDS SO WE DO LIKE THIS\n","        common_stopwords = set(words1) & set(words2)\n","\n","        # Count the number of common stopwords\n","        num_common_stopwords = len(common_stopwords.intersection(stop_words))\n","\n","        num_common_words=num_common_words-num_common_stopwords\n","\n","        #get min number of words' question\n","        num_stopwords1 = sum(word in stop_words for word in words1)\n","        num_stopwords2 = sum(word in stop_words for word in words2)\n","\n","        max_len=max( len(str(text1).lower().split()) - num_stopwords1,\n","                    len(str(text2).lower().split()) - num_stopwords2)\n","        if max_len==0:\n","          cwc_max=num_common_words/1\n","        else:\n","          cwc_max=num_common_words/max_len\n","\n","        return cwc_max\n","      except (TypeError, KeyError):\n","        print(data)\n","        # Handle the case where the data is not of the expected type or the expected keys are missing\n","        return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2O427RZWkJ9w"},"outputs":[],"source":["df['cwc_max']=df.apply(cwc_max,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"MggP3GC04weT"},"source":["**Adding feature of last word equal**\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIiJl6X55Dar"},"outputs":[],"source":["def l_word_equal(data):\n","      text1 = data[\"question1\"]\n","      text2 = data[\"question2\"]\n","      # Tokenize the texts into words\n","      words1 = str(text1).lower().split()\n","      words2 = str(text2).lower().split()\n","      if words1 and words2:\n","        if words1[-1]==words2[-1]:\n","          return 1\n","        else:\n","          return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TnJ9ZM35e7M"},"outputs":[],"source":["df['last_word_eq']=df.apply(l_word_equal,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"BWRebaluDJzw"},"source":["**Adding feature of first word equal**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqA6hzqWDNG_"},"outputs":[],"source":["def f_word_equal(data):\n","      text1 = data[\"question1\"]\n","      text2 = data[\"question2\"]\n","      # Tokenize the texts into words\n","      words1 = str(text1).lower().split()\n","      words2 = str(text2).lower().split()\n","      if words1 and words2:\n","        if words1[-1]==words2[-1]:\n","          return 1\n","        else:\n","          return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6RbRZIaDQX_"},"outputs":[],"source":["df['first_word_eq']=df.apply(f_word_equal,axis=1)"]},{"cell_type":"markdown","metadata":{"id":"pROHPBjmEqCC"},"source":["**Now let's apply basic preprocessing\n","We will remove stop words,use stemming,convert ain't to are not and stuff like this and also try to remove HTML tags and will convert all letters to lower case in upcoming cells**   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gLdpC2PyFdCT"},"outputs":[],"source":["def preprocess(q1):\n","    import nltk\n","    from nltk.corpus import stopwords\n","    from nltk.stem import PorterStemmer\n","    from nltk.tokenize import word_tokenize\n","    from nltk.tokenize import RegexpTokenizer\n","    import re\n","\n","    # Set the language for stopwords\n","    stop_words = set(stopwords.words('english'))\n","\n","    pattern = r'\\[math\\]|\\w+|\\$[\\d\\.]+'\n","\n","    # Create the custom tokenizer\n","    tokenizer = RegexpTokenizer(pattern)\n","            #Converting mappings to full words\n","    word_mapping = {\n","    \"can't\": \"cannot\",\n","    \"couldn't\": \"could not\",\n","    \"didn't\": \"did not\",\n","    \"doesn't\": \"does not\",\n","    \"don't\": \"do not\",\n","    \"hadn't\": \"had not\",\n","    \"hasn't\": \"has not\",\n","    \"haven't\": \"have not\",\n","    \"he's\": \"he is\",\n","    \"I'd\": \"I would\",\n","    \"I'll\": \"I will\",\n","    \"I'm\": \"I am\",\n","    \"I've\": \"I have\",\n","    \"isn't\": \"is not\",\n","    \"it's\": \"it is\",\n","    \"let's\": \"let us\",\n","    \"mightn't\": \"might not\",\n","    \"mustn't\": \"must not\",\n","    \"shan't\": \"shall not\",\n","    \"she's\": \"she is\",\n","    \"shouldn't\": \"should not\",\n","    \"that's\": \"that is\",\n","    \"there's\": \"there is\",\n","    \"they're\": \"they are\",\n","    \"they've\": \"they have\",\n","    \"we'd\": \"we would\",\n","    \"we're\": \"we are\",\n","    \"we've\": \"we have\",\n","    \"weren't\": \"were not\",\n","    \"what's\": \"what is\",\n","    \"won't\": \"will not\",\n","    \"wouldn't\": \"would not\",\n","    \"you'd\": \"you would\",\n","    \"you're\": \"you are\",\n","    \"you've\": \"you have\",\n","    \"\\'m\":\"km\",\n","    \"[math]\": \"Mathematical Expression or formula\"\n","    # Add more mappings as needed\n","}\n","    if not isinstance(q1, str):\n","        q1 = str(q1)\n","# Replace contractions with their expanded forms in the sentence\n","    for contraction, expanded_form in word_mapping.items():\n","        q1 = q1.replace(contraction, expanded_form)\n","\n","#removing digits\n","    pattern = r'\\d+'\n","\n","    # Use the sub() function to remove digits\n","    q1 = re.sub(pattern, '', q1)\n","\n","    # Tokenize the sentence\n","    tokens1 = tokenizer.tokenize(q1)\n","\n","    q2=[]\n","    for word in  tokens1:\n","       if word in word_mapping:\n","          word=word_mapping[word]\n","       q2.append(word)\n","    q1=q2\n","\n","    #Make object of stemmer\n","    stemmer = PorterStemmer()\n","\n","\n","    #Apply stemmer ,this also converts words to lower case and also remove stop words\n","    q1=\" \".join([stemmer.stem(word) for word in q1 if word.lower() not in stop_words])\n","    return q1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3349,"status":"ok","timestamp":1688812889244,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"O7qOFbdj1DtW","outputId":"ace6eac8-b5b3-43b3-9a4f-ce364622d396"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-37-46ff889b56b7>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.config.list_physical_devices('GPU')` instead.\n"]},{"name":"stdout","output_type":"stream","text":["False\n"]}],"source":["import tensorflow as tf\n","print(tf.test.is_gpu_available())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4S95B6Y2SMT_"},"outputs":[],"source":["df['question1']=df['question1'].apply(preprocess)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y6A72_9HJaGr"},"outputs":[],"source":["df['question2']=df['question2'].apply(preprocess)"]},{"cell_type":"markdown","metadata":{"id":"DHOQpprKmy0W"},"source":["**Basic preprocessing done and now no need to remove html tags**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1688812963752,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"EKqb2P1doFEl","outputId":"e1e9df78-f2ee-4143-c3fb-cbc6243a0a50"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-eae83113-aad1-45b2-a32c-da10998f4489\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>qid1</th>\n","      <th>qid2</th>\n","      <th>question1</th>\n","      <th>question2</th>\n","      <th>is_duplicate</th>\n","      <th>csc_min</th>\n","      <th>csc_max</th>\n","      <th>ctc_min</th>\n","      <th>ctc_max</th>\n","      <th>longest_substr_ratio</th>\n","      <th>mean_length</th>\n","      <th>abs_len_diff</th>\n","      <th>partial_ratio</th>\n","      <th>token_sort_ratio</th>\n","      <th>token_set_ratio</th>\n","      <th>cwc_min</th>\n","      <th>cwc_max</th>\n","      <th>last_word_eq</th>\n","      <th>first_word_eq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eae83113-aad1-45b2-a32c-da10998f4489')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-eae83113-aad1-45b2-a32c-da10998f4489 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-eae83113-aad1-45b2-a32c-da10998f4489');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["Empty DataFrame\n","Columns: [id, qid1, qid2, question1, question2, is_duplicate, csc_min, csc_max, ctc_min, ctc_max, longest_substr_ratio, mean_length, abs_len_diff, partial_ratio, token_sort_ratio, token_set_ratio, cwc_min, cwc_max, last_word_eq, first_word_eq]\n","Index: []"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["df[df[\"ctc_min\"]!=df[\"cwc_min\"]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0wDASy23QuZ"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1688812963753,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"eH6xsJfmodV2","outputId":"18c3a266-b116-4b72-bfee-1793223f525b"},"outputs":[{"data":{"text/plain":["5"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["\n","data={\n","    \"question2\" : \"Why am I mentally very lonely?How can I solve.\",\n","    \"question1\" : \"How can I go to gym?\"}\n","abs_len_diff(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1688812963753,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"QWA2gbmjPwht","outputId":"25a932da-d936-4057-99a4-3aa238b02976"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-673f1f65-ee9c-458b-9586-a394a0f9f98a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>qid1</th>\n","      <th>qid2</th>\n","      <th>question1</th>\n","      <th>question2</th>\n","      <th>is_duplicate</th>\n","      <th>csc_min</th>\n","      <th>csc_max</th>\n","      <th>ctc_min</th>\n","      <th>ctc_max</th>\n","      <th>longest_substr_ratio</th>\n","      <th>mean_length</th>\n","      <th>abs_len_diff</th>\n","      <th>partial_ratio</th>\n","      <th>token_sort_ratio</th>\n","      <th>token_set_ratio</th>\n","      <th>cwc_min</th>\n","      <th>cwc_max</th>\n","      <th>last_word_eq</th>\n","      <th>first_word_eq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>99995</th>\n","      <td>99995</td>\n","      <td>165922</td>\n","      <td>165923</td>\n","      <td>icon imag footbal</td>\n","      <td>icon imag women</td>\n","      <td>0</td>\n","      <td>0.857143</td>\n","      <td>0.857143</td>\n","      <td>0.818182</td>\n","      <td>0.818182</td>\n","      <td>9</td>\n","      <td>11.0</td>\n","      <td>0</td>\n","      <td>90</td>\n","      <td>85</td>\n","      <td>93</td>\n","      <td>0.818182</td>\n","      <td>0.666667</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>99996</th>\n","      <td>99996</td>\n","      <td>165924</td>\n","      <td>165925</td>\n","      <td>green green tea</td>\n","      <td>green tea green</td>\n","      <td>0</td>\n","      <td>0.500000</td>\n","      <td>0.500000</td>\n","      <td>0.666667</td>\n","      <td>0.444444</td>\n","      <td>1</td>\n","      <td>7.5</td>\n","      <td>3</td>\n","      <td>48</td>\n","      <td>77</td>\n","      <td>72</td>\n","      <td>0.666667</td>\n","      <td>0.666667</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>99997</th>\n","      <td>99997</td>\n","      <td>165926</td>\n","      <td>165927</td>\n","      <td>would win black panther batman</td>\n","      <td>would win fight black panther batman</td>\n","      <td>1</td>\n","      <td>0.500000</td>\n","      <td>0.200000</td>\n","      <td>0.777778</td>\n","      <td>0.583333</td>\n","      <td>3</td>\n","      <td>10.5</td>\n","      <td>3</td>\n","      <td>64</td>\n","      <td>74</td>\n","      <td>96</td>\n","      <td>0.777778</td>\n","      <td>0.833333</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>99998</th>\n","      <td>99998</td>\n","      <td>165928</td>\n","      <td>165929</td>\n","      <td>school better parson risd</td>\n","      <td>good design school colleg par risd</td>\n","      <td>0</td>\n","      <td>0.200000</td>\n","      <td>0.142857</td>\n","      <td>0.272727</td>\n","      <td>0.200000</td>\n","      <td>1</td>\n","      <td>13.0</td>\n","      <td>4</td>\n","      <td>46</td>\n","      <td>51</td>\n","      <td>51</td>\n","      <td>0.272727</td>\n","      <td>0.166667</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>99999</th>\n","      <td>99999</td>\n","      <td>165930</td>\n","      <td>165931</td>\n","      <td>human bodi produc carbon dioxid</td>\n","      <td>would inhal carbon dioxid caus human</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.500000</td>\n","      <td>0.400000</td>\n","      <td>2</td>\n","      <td>9.0</td>\n","      <td>2</td>\n","      <td>49</td>\n","      <td>52</td>\n","      <td>65</td>\n","      <td>0.500000</td>\n","      <td>0.500000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-673f1f65-ee9c-458b-9586-a394a0f9f98a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-673f1f65-ee9c-458b-9586-a394a0f9f98a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-673f1f65-ee9c-458b-9586-a394a0f9f98a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["          id    qid1    qid2                        question1  \\\n","99995  99995  165922  165923                icon imag footbal   \n","99996  99996  165924  165925                  green green tea   \n","99997  99997  165926  165927   would win black panther batman   \n","99998  99998  165928  165929        school better parson risd   \n","99999  99999  165930  165931  human bodi produc carbon dioxid   \n","\n","                                  question2  is_duplicate   csc_min   csc_max  \\\n","99995                       icon imag women             0  0.857143  0.857143   \n","99996                       green tea green             0  0.500000  0.500000   \n","99997  would win fight black panther batman             1  0.500000  0.200000   \n","99998    good design school colleg par risd             0  0.200000  0.142857   \n","99999  would inhal carbon dioxid caus human             0  0.000000  0.000000   \n","\n","        ctc_min   ctc_max  longest_substr_ratio  mean_length  abs_len_diff  \\\n","99995  0.818182  0.818182                     9         11.0             0   \n","99996  0.666667  0.444444                     1          7.5             3   \n","99997  0.777778  0.583333                     3         10.5             3   \n","99998  0.272727  0.200000                     1         13.0             4   \n","99999  0.500000  0.400000                     2          9.0             2   \n","\n","       partial_ratio  token_sort_ratio  token_set_ratio   cwc_min   cwc_max  \\\n","99995             90                85               93  0.818182  0.666667   \n","99996             48                77               72  0.666667  0.666667   \n","99997             64                74               96  0.777778  0.833333   \n","99998             46                51               51  0.272727  0.166667   \n","99999             49                52               65  0.500000  0.500000   \n","\n","       last_word_eq  first_word_eq  \n","99995             0              0  \n","99996             0              0  \n","99997             1              1  \n","99998             1              1  \n","99999             0              0  "]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["df.tail()"]},{"cell_type":"markdown","metadata":{"id":"twN0RtkaPY6-"},"source":["**Apply Word2Vec** *solve using try and except here the problem*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5889,"status":"ok","timestamp":1688812969614,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"jYZxhGgUPb-z","outputId":"6f6de8ea-d2b1-4138-a208-7d6714504803"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["\n","from gensim.models import Word2Vec\n","import nltk\n","from nltk import sent_tokenize\n","nltk.download('punkt')\n","input=[]\n","\n","def make_corpus(data):\n"," for i in data:\n","   import re\n","   punctuation_pattern = r'[^\\w\\s]'\n","\n","    # Remove punctuation marks using regular expression substitution\n","    # Example texts\n","   i = re.sub(punctuation_pattern, ' ', i)\n","   i=str(i).lower()\n","   raw_sent=sent_tokenize(i)\n","   for sent in raw_sent:\n","       input.append(sent.split())\n","make_corpus(df['question1'])\n","make_corpus(df['question2'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10582,"status":"ok","timestamp":1688812980189,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"IFHFeDuSoFXo","outputId":"84237963-b1de-4752-ef03-d78e005f2d9a"},"outputs":[{"data":{"text/plain":["(5147155, 5413970)"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["model=Word2Vec(vector_size=100,window=10,min_count=1)\n","model.build_vocab(input)\n","model.train(input,total_examples=model.corpus_count,epochs=model.epochs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zg69DdKKrGKK"},"outputs":[],"source":["import numpy as np\n","\n","def transform(data):\n","    import re\n","    if data is None:\n","        print(\"I am empty\")\n","        return [0] * 100\n","\n","    from nltk.corpus import stopwords\n","    # Get the list of stopwords for English\n","    stop_words = set(stopwords.words('english'))\n","\n","    # Define the pattern for punctuation marks\n","    punctuation_pattern = r'[^\\w\\s]'\n","\n","    # Remove punctuation marks using regular expression substitution\n","    data = re.sub(punctuation_pattern, ' ', data)\n","\n","    # Tokenize the texts into words\n","    words = str(data).lower().split()\n","    for t in range(len(words)):\n","        if words[t] in model.wv:\n","            words[t] = model.wv[words[t]]\n","        else:\n","            print(words[t])\n","\n","    data = words\n","    len1 = len(data)\n","\n","    lists = []\n","    for lst in data:\n","        lst = lst.astype(float)\n","        lists.append(lst)\n","\n","    if len(lists) > 0:\n","        return np.mean(lists, axis=0).tolist()\n","    else:\n","        return [0] * 100\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMo3-4VzsaAU"},"outputs":[],"source":["df['question1']=df['question1'].apply(transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6R7yhjAV51Q5"},"outputs":[],"source":["df['question2']=df['question2'].apply(transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1688813032759,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"wo0LIGsksaGZ","outputId":"8b92c97a-445b-4abc-f195-03ff03d12a1c"},"outputs":[{"data":{"text/plain":["100"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["len(df['question1'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZMtVp-w5tbX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Tdg8ipTT3PZ"},"outputs":[],"source":["import pandas as pd\n","\n","feature_columns_1 = []\n","feature_columns_2 = []\n","\n","for i in range(100):\n","        feature_columns_1.append(df['question1'].apply(lambda x: x[i] if x is not None else 0).rename(f'feature_q1_{i}'))\n","        feature_columns_2.append(df['question2'].apply(lambda x: x[i] if x is not None else 0).rename(f'feature_q2_{i}'))\n","\n","df = pd.concat([df] + feature_columns_1, axis=1)\n","df = pd.concat([df] + feature_columns_2, axis=1)\n","\n","# df.drop(['question1', 'question2'], axis=1, inplace=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJ3q8YlsRl83"},"outputs":[],"source":["df.drop(['question1', 'question2'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1688813048119,"user":{"displayName":"Sohaib ahmed","userId":"16177918511967355559"},"user_tz":-300},"id":"jyzr7dRvQLbG","outputId":"6a4d41a3-e148-4620-9e7c-cb3a03c457ae"},"outputs":[{"data":{"text/plain":["Index(['id', 'qid1', 'qid2', 'is_duplicate', 'csc_min', 'csc_max', 'ctc_min',\n","       'ctc_max', 'longest_substr_ratio', 'mean_length',\n","       ...\n","       'feature_q2_90', 'feature_q2_91', 'feature_q2_92', 'feature_q2_93',\n","       'feature_q2_94', 'feature_q2_95', 'feature_q2_96', 'feature_q2_97',\n","       'feature_q2_98', 'feature_q2_99'],\n","      dtype='object', length=218)"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69ywjZsQ87g-"},"outputs":[],"source":["X = df.drop(\"is_duplicate\", axis=1)\n","Y = df[\"is_duplicate\"]"]},{"cell_type":"markdown","metadata":{"id":"bOM8HLvPOrVW"},"source":["**Train Test Split**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uc4AaZiAOtlM"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the data into train and test sets\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"PH7AiFHcO35g"},"source":["**Apply Random Forest**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trDLAAppOuUK"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Create a Random Forest classifier\n","rf_classifier = RandomForestClassifier(max_depth=2,n_estimators=50)\n","\n","# Train the classifier on the training data\n","rf_classifier.fit(X_train, Y_train)\n","\n","# Make predictions on the test data\n","predictions = rf_classifier.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzk_OW-ShpHU"},"outputs":[],"source":["print(Y_train.isnull().sum())\n","Y_train = Y_train.fillna(0)\n","Y_test = Y_test.fillna(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MOzHk1X8SoYL"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(Y_test, predictions)\n","print(\"Accuracy:\", accuracy*100)\n"]},{"cell_type":"markdown","metadata":{"id":"DdaJPdNBSqcO"},"source":["**Checking Overfitting**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMgfs7CJRe0_"},"outputs":[],"source":["import random\n","indices = random.sample(range(len(X_train)), int(len(X_train) * 0.2))\n","X_train_subset = X_train.iloc[indices]\n","Y_train_subset = Y_train.iloc[indices]\n","\n","# Make predictions on the test data\n","predictions = rf_classifier.predict(X_train_subset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fcef22aApY8w"},"outputs":[],"source":["print(Y_train.isnull().sum())\n","Y_train = Y_train.fillna(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COolekLsSCcv"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(Y_train_subset, predictions)\n","print(\"Accuracy:\", accuracy*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gyM7acEdO7WS"},"outputs":[],"source":["df[\"feature_q1_0\"].info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8rzDr3oRh_n"},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"x1oTTCBzSK6O","outputId":"8f9c5f66-dd19-45ae-af53-9e402caa7ad7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 77.605\n"]}],"source":["import xgboost as xgb\n","from sklearn.metrics import accuracy_score\n","\n","# Define XGBoost classifier and set hyperparameters\n","xgb_classifier = xgb.XGBClassifier(\n","    n_estimators=100,\n","    max_depth=3,\n","    learning_rate=0.1,\n","    subsample=0.8,\n","    colsample_bytree=0.8\n",")\n","\n","# Train the XGBoost classifier using the training data\n","xgb_classifier.fit(X_train, Y_train)\n","\n","# Make predictions on the test data\n","predictions = xgb_classifier.predict(X_test)\n","\n","# Calculate accuracy of predictions\n","accuracy = accuracy_score(Y_test, predictions)\n","print(\"Accuracy:\", accuracy*100)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VqJ-6pxPqv8D"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1rlhm8wncWlnR7qwjePIn0TTW0OEs_r8j","authorship_tag":"ABX9TyN3hywZPGCwIW2quqgQLQZk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}